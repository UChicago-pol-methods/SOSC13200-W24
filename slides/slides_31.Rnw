\documentclass[xcolor={dvipsnames}]{beamer}

\usepackage{../assets/pres-template_MOW}
\usepackage{verbatim}

%--------------------------------------------------------------------------
% Specific to this document ---------------------------------------
%--------------------------------------------------------------------------
% \setbeamercovered{transparent}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\tabcolsep}{1.3pt}
\title{Social Science Inquiry II}
\subtitle{Week 3: A brief introduction to probability}
\date{Winter 2023}
\author{Molly Offer-Westort}
\institute{Department of Political Science, \\University of Chicago}


\begin{document}
\SweaveOpts{concordance=TRUE, prefix.string = figs31/}

\nocite{aronow2019foundations,wasserman2004all,hernan2010causal}

%-------------------------------------------------------------------------------%
<<setup, include=FALSE, echo=FALSE>>=
if(!dir.exists('figs31')){dir.create('figs31')}
@

%-------------------------------------------------------------------------------%
\frame{\titlepage
\thispagestyle{empty}
}
%-------------------------------------------------------------------------------%
\begin{frame}{Housekeeping}

\begin{itemize}
\item Homework assignments
  \begin{itemize}
    \item Feedback is very helpful (thank you!)
    \item If you spend $>30$ minutes without making progress, go to Stack Overflow or solutions.
  \end{itemize}
\end{itemize}

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}[fragile]{Loading packages for this class}

<<>>=
library(ggplot2)
library(gridExtra)
set.seed(60637)
@
%-------------------------------------------------------------------------------%
\end{frame}

\begin{frame}{What do we do with data}

Now that we've gotten started on working with data \dots what do we want to get from that data?

\pause
\begin{itemize}
\item Describe what's going on in the data \pause
\item We can do a pretty good job of this with the data visualization tools we have, along with summary statistics for numerical descriptions
\end{itemize}
\pause

This is already a really useful start, but beyond just describing the data we see in front of us, we may have other goals. We may want to:\pause

\begin{itemize}
\item Make generalizations to a larger population \pause
\item Make informed guesses about how things would have turned out differently, if something different had happened \pause
\end{itemize}
These latter two are called \textit{inference}, and we'll need some additional tools and assumptions to make headway on them

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}{Probability}

Why do we use probability theory? \pause

\begin{itemize}
\item Probability theory allows us to talk about \textit{random} events in structured way. \pause
\begin{itemize}
\item Often, we see only part of the picture we'd like to talk about. We only see some of the data, or we can only see one version of events. \pause
\item Probability theory gives us a way to describe the process that results in the data that we observe. \pause
\item It also allows us to \textit{formalize our uncertainty} when making inference.
\end{itemize}
\end{itemize}

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}{Randomness}

What does it mean to describe real world events as ``random''? \pause

\begin{itemize}
\item Probability theory is an abstract construct, but it is useful for empirical research to create a model of the world in which events are probabilistic. \pause
\begin{itemize}
\item If we are conducting measurement on some population but can only observe a sample, we assume that there is some randomness to who we observe and who we don't observe.\pause
\end{itemize}
\item If we are describing events with counterfactuals, such as turning out to vote or testing positive for COVID, it can be useful to describe those events as probabilistic.\pause
\item These are \textit{models} of how the world works, and they help us make sense of the fundamentally squishy nature of social science research. With limited information about the world, we operate with uncertainty. Assigning probabilities conditional on the information we \textit{do} have helps us formalize that uncertainty.  \pause Even if we don't necessarily believe that human behavior is ``random.''
\end{itemize}

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}{Flipping a coin twice}

Suppose we are flipping a coin twice, and the coin is fair. This is a random process, and we will describe the probability space associated with this process.

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}[label = terms]{Useful terms in probability}


\pause
\begin{itemize}
  \item $\Omega$ : Sample space. Describes all possible outcomes in our setting.\pause
  \begin{itemize}
   \item $\omega$ : Generic notation for the realized outcomes in the sample space. \pause
   \item Here, $\Omega = \{HH, HT, TH, TT \}$. \pause
  \end{itemize}
\item Event: a subset of $\Omega$. \pause
\begin{itemize}
 \item We will often use terms like $A$ or $B$ to define events. \pause
 \item Here, the event that we get a head on first flip is $A = \{HT, HH\}$. \pause
\end{itemize}
\item $S$ : Event space. Describes all subsets of events, including null set. \hyperlink{event_space}{\beamerbutton{Full event space}}\pause
\begin{itemize}
 \item We use this in addition to the sample space, so we can describe all types of events that we can define the probability for. \pause
\end{itemize}
\item $\textrm{P}$ : Probability measure. An operator that assigns probability to all events in the event space. \pause
\begin{itemize}
 \item Here, the event that we get a head on the first flip, $\textrm{P}[A] = 1/2$.
 \end{itemize}
\end{itemize}

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item In probability, we often use uppercase terms to denote the random process, and lowercase terms to denote specific outcomes of that random process.
\end{itemize}

}

%-------------------------------------------------------------------------------%

\begin{frame}[fragile]{An aside on mathematical notation}\small

Mathematical notation is a tool that gives us a common language to express concepts with precision. There are different conventions in different communities, and no approach is ``right'' or ``wrong''--it's just a question of whether your notation is appropriately communicating to your audience what you want it to. \\~\ \pause 

To use mathematical notation in R Markdown, write LaTeX typesetting commands inside of dollar signs. \\~\ \pause 

For example,

\begin{verbatim}
$Y = \beta_0 + \beta_1 X$
\end{verbatim}\\~\

\pause 

is rendered as \\~\

$Y = \beta_0 + \beta_1 X$.\\~\ \pause


We have just used probability notation as a way to fully describe any random generative process. 

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}[fragile]{Sampling}
\scriptsize

We can simulate our double coin flip process in R, using the \texttt{sample()} function. There are four possible outcomes, and all are equally likely.

<<>>=
Omega <- c('HH', 'HT', 'TH', 'TT')
probs <- c(0.25, 0.25, 0.25, 0.25)

sample(x = Omega,
       size = 1,
       prob = probs)
@


\pause
We can run this simulation many times, and our results should \textit{approximately} follow the probabilities we assigned.

<<>>=
n <- 1000
result_n <- sample(x = Omega,
       size = n,
       prob = probs,
       replace = TRUE)

table(result_n)
@



\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}{Independent events}

Two events are \textit{independent} if
$$
\textrm{P}[AB] = \textrm{P}[A]\textrm{P}[B]
$$\\~\
\pause

\textit{Notational aside: The event $AB$ is that both $A$ and $B$ happen. There are other ways to write this, including $A \cap B$.}


\end{frame}

%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%

\begin{frame}

Returning to our example of flipping two fair coins. Let's say:

\begin{itemize}
\item Event $A$: we get a head on the first coin flip; $A = \{HT, HH\}$.
\item Event $B$: we get a head on the second coin flip; $B = \{TH, HH\}$.
\item We can see the event $AB$ as the overlap in their respective sets, $AB = \{HH\}$
\end{itemize}

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}
\scriptsize

The coin flips are unrelated, so the events should be independent. We can check this mathematically.

\pause

First, we know that all of the outcomes $\Omega = \{HH, HT, TH, TT \}$ are equally likely.

\begin{align*}
\textrm{P}[A] &= \textrm{P}[\{HT\}] + \textrm{P}[\{HH\}] = 0.25 + 0.25 = 0.5\\\
\textrm{P}[B] &= \textrm{P}[\{TH\}] + \textrm{P}[\{HH\}] = 0.25 + 0.25 = 0.5
\end{align*}


\pause
Then, we can calculate the product of the probabilities, and the probability of the joint event.

\begin{align*}
\textrm{P}[A]\textrm{P}[B] &= 0.5 \times 0.5 = 0.25\\\
\textrm{P}[AB] &= \textrm{P}[\{HH\}] = 0.25\
\end{align*}


\pause
We see that they are the same, so we have independence.

$$
\textrm{P}[A]\textrm{P}[B] = \textrm{P}[AB]
$$


\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}[fragile]
\scriptsize

We can also check if observed proportions in our simulations show the same thing\pause

<<>>=
Omega <- c('HH', 'HT', 'TH', 'TT');  probs <- c(0.25, 0.25, 0.25, 0.25)
result_n <- sample(x = Omega,
                   size = n,
                   prob = probs,
                   replace = TRUE)

(observed_props <- prop.table(table(result_n)))
@

\pause
<<>>=
(PA <- mean(result_n == 'HT' | result_n == 'HH'))
@

\pause
<<>>=
(PB <- mean(result_n == 'TH' | result_n == 'HH'))
@

\pause
<<>>=
(PAB <- mean(result_n == 'HH'))
PA*PB
@

The proportions look pretty close.

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}[fragile]

{What about for a case where $A$ and $B$ are not independent?}

\small 

\pause
\begin{itemize}
\item $A$ is still the event we get a head on the first coin flip; $A = \{HT, HH\}$.
\item $B$ is now the event that we get a head on both coin flips; $B = \{HH\}$.
\item $AB$ is the intersection of these two sets, which is just $AB = \{HH\}$.
\end{itemize} 


\ \ ~\

\scriptsize

\pause
<<>>=
(observed_props <- prop.table(table(result_n)))
@

\pause
<<>>=
(PA <- mean(result_n == 'HT' | result_n == 'HH'))
@

\pause
<<>>=
(PB <- mean(result_n == 'HH'))
@

\pause
<<>>=
(PAB <- mean(result_n == 'HH'))
PA*PB
@


\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}{Conditional probability}


If $\textrm{P}[B]>0$, the \textit{conditional probability} of an event $A$ occurring, given event $B$ has occurred is:

$$
\textrm{P}[A|B] = \frac{\textrm{P}[AB]}{\textrm{P}[B]}
$$
\pause This can also be read as, out of all of the times event $B$ occurs, how many times does event $A$ also occur?

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}

For our coin flip example, we'll stick with:
\begin{itemize}
\item $A$ is  the event that we get a head on the first coin flip; $A = \{HT, HH\}$.
\item $B$ is the event that we get a head on the both coin flips; $B = \{HH\}$.
\item $A  B$ is $\{HH\}$
\end{itemize}

\pause
\begin{align*}
\textrm{P}[A|B]& = \frac{\textrm{P}[AB]}{\textrm{P}[B]}\\\
& = \frac{\textrm{P}[\{ HH \}]}{\textrm{P}[\{ HH \}]}\\\
& = 1
\end{align*}

\pause
What is $\textrm{P}[B|A]$?

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}{Bayes Rule}

A useful theorem to return to is \textit{Bayes Rule}

$$
\textrm{P}[A|B] = \frac{\textrm{P}[B|A]\textrm{P}[A]}{\textrm{P}[B]}
$$

\pause Why is Bayes rule so useful?
\pause It basically tells us how we update probability based on observed data.

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}{Bayes Rule}


Example: Suppose everyone in the University of Chicago community is given a new rapid test for COVID.

\begin{itemize}
\item We are concerned with \textit{false negatives}, when we get a negative test for a person who actually \textit{is} infected.
\item And \textit{false positives}, when we get a positive test for a person who \textit{is not} infected.
\end{itemize}

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}{Bayes Rule}


Example: Suppose everyone in the University of Chicago community is given a new rapid test for COVID.\\~\

A student gets a positive test back. What is the probability that they have COVID?

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}{Bayes Rule}


\begin{itemize}
\item Event $A$: person has COVID
\item Event $B$:  a positive test
\end{itemize}

\pause

Should we think these events are independent?

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}[fragile]{Bayes Rule}


\begin{itemize}
\item Event $A$: person has COVID
\item Event $B$:  a positive test
\end{itemize}

\pause


Doctors know that the probability of having COVID is 3\% in this population : $\textrm{P}[A]$
<<>>=
PA <- 0.03
@
\pause

The overall rate of positive tests is 5\% : $\textrm{P}[B]$
<<>>=
PB <- 0.05
@

\pause
If you have COVID, your test will turn up positive 95\% of the time : $\textrm{P}[B|A]$
<<>>=
PB_if_A <- 0.95
@


\pause
\begin{align*}
\textrm{P}[A|B] & = \frac{\textrm{P}[B|A]\textrm{P}[A]}{\textrm{P}[B]}
\end{align*}


\pause
<<>>=
( PB_if_A * PA )/PB
@


\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}{Prosecutor's fallacy}

$$
\textrm{P}[A|B]\overset{?}= \textrm{P}[B|A]
$$

\pause
As a general rule, we cannot assume the conditional probability of $A$ given $B$ is the same as the probability of $B$ given $A$
\pause

\begin{itemize}
 \item In a court case, the probability that a person is guilty given that we see a DNA match is NOT the same as the probability of a DNA match given  that they are guilty.
\pause
\item This fallacy often occurs when we observe evidence that we are very unlikely to see among innocent people, and very likely to observe among guilty people
\pause
\item But the overall number of innocent people in the population is very large, so the number of false positives is much higher than the number of true positives
\end{itemize}

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}{Why thinking about conditional probability is so important in social science research}

``20\% of people hospitalized with COVID-19 are vaccinated''

\pause
Does this tell us that vaccines aren't very effective?

\pause
Some things we should think about:
\begin{itemize}
\item What percentage of the population in areas served by hospitals are vaccinated?
\item Are people who are vaccinated at higher risk for breakthrough cases? Or more likely to be hospitalized? Are they older, or have pre-existing conditions?
\end{itemize}


\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}{Why thinking about conditional probability is so important in social science research}



What data do you need to answer the question: ``Do white officers shoot minority citizens at a higher rate than non-white officers?''

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}

Remember this article?

\begin{figure}[htbp]
   \centering
   \includegraphics{../assets/npr.png} 
\end{figure}

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}

Remember this article?

\begin{figure}[htbp]
   \centering
   \includegraphics{../assets/guardian.png} 
\end{figure}



\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}

\begin{figure}[htbp]
   \centering
   \includegraphics{../assets/johnson-et-al-PNAS.png} 
\end{figure}
\small

\begin{itemize}
\item In a 2020 PNAS paper, Johnson and co-authors evaluated the relationship between race of victims shot by police, and the characteristics of the police shooters
\pause
\item The authors claim, ``White officers are not more likely to shoot minority civilians than non-White officers''--and the paper has been used in congressional testimony to support the claim that diversity in police forces would not be beneficial in reducing bias in officer-involved shootings
\pause
\item Their database only has data on fatal shootings--not on cases where people were NOT shot.
\item Their results report ``whether a person fatally shot was more likely to be Black (or Hispanic) than White'' conditional on the race of the officer involved
\pause
\item Does this address original claim?
\end{itemize}


\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}

``White officers are not more likely to shoot minority civilians than non-White officers''

\begin{align*}
\textrm{P}[\text{shot} | \text{White officer, minority civilian}] -\\
\textrm{P}[\text{shot} | \text{Minority officer, minority civilian}]
\end{align*}

What data are we missing to address this question?

\pause

\begin{itemize}
\item Dean Knox \& Jonathan Mummolo wrote a letter critiquing the original article, based on the authors' failure to appropriately apply Bayes Rule;\pause
\item The article was eventually retracted \pause
\item (It's a bit more complicated than that, but know your conditional probability when thinking about difficult subjects!)
\end{itemize}


\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%

\begin{frame}{Random variables}

\begin{itemize}
\item A random variable is a mapping $X$ from our sample space $\Omega$, to the Real numbers.
$$X : \Omega \to {\rm I\!R}$$\pause
\item Random variables are ways to quantify random events described by our sample space. \pause
\item We'll mostly work with random variables going forward, but it's important to remember that the random variable is built on the foundations of the sample space -- and often, \textbf{you'll be the one deciding how that quantification happens}.
\end{itemize}

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}

For example, with our two coin flips, let $X(\omega)$ be the number of heads in the sequence $\omega$.
\pause

Then the random variable, and its probability distribution, can be described as:
\begin{table}[]
\begin{tabular}{ccc}
$\omega$ & $\textrm{P}[\{\omega\}]$ & $X(\omega)$ \\ \hline
TT & 1/4 & 0 \\
TH & 1/4 & 1 \\
HT & 1/4 & 1 \\
HH & 1/4 & 2
\end{tabular}
\end{table}

and,

\begin{table}[]
\begin{tabular}{cc}
$x$ & $\textrm{P}[X = x]$ \\ \hline
0 & 1/4 \\
1 & 1/2 \\
2 & 1/4
\end{tabular}
\end{table}


\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


% -------------------------------------------------------------------------------%
\begin{frame}[fragile]

\scriptsize

We can simulate this in `R` as well.

\pause

<<>>=
X <- c(0, 1, 2)
probs <- c(0.25, 0.5, 0.25)

sample(x = X,
       size = 1,
       prob = probs)

@

\pause

<<>>=
n <- 1000
result_n <- sample(x = X,
                   size = n,
                   prob = probs,
                   replace = TRUE)

table(result_n)
@

\pause


<<>>=

prop.table(table(result_n))

@

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%
\begin{frame}[fragile]

\scriptsize
We can plot a histogram to look at the distribution of results.
\tiny

\begin{figure}
\centering
<<fig = TRUE, width = 5, height = 3.5>>=
ggplot(data.frame(result_n), aes(x = result_n)) +
  geom_histogram(bins = 3, color = 'white', fill = 'lightgreen') +
  theme_bw() + xlab('Number heads')

@
\end{figure}
\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}{Probability Mass Function of a discrete random variable}


\begin{itemize}
\item A random variable is \textit{discrete} if it takes countably many values.\pause
\item The probability mass function of a discrete RV $X$ tells us the probability we will see an outcome at some value $x$. \pause
\end{itemize}

$$
f(x) = \textrm{P}[X = x]
$$

\pause

For our coin flip example,


$$
f(x) = \begin{cases}
1/4 & x = 0 \\
1/2 & x = 1 \\
1/4 & x = 2 \\
0 & \text{otherwise}
\end{cases}
$$

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}[fragile]{Illustrating the PMF of a discrete RV}
\tiny
\begin{figure}
\centering
<<fig = TRUE, width = 5, height=3.8, echo=FALSE>>=
plotdata <- data.frame(
  x = c(-1, 0, 1, 2),
  xend = c(0, 1, 2, 3),
  fx = c(0, 1/4, 1/2, 1/4),
  Fx = c(0, 1/4, 3/4, 1) # cumsum(fx)
)

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.5, 2.5),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0,0,0), xend = x, yend = fx)) +
  ggtitle('PMF of X as number of heads in 2 fair coin flips') +
  theme_bw()
@
\end{figure}
\pause

Note that the probabilities sum to 1. This is one of the foundational axioms of probability.

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}{Cumulative Distribution Functions}

The cumulative distribution function of $X$ tells us the probability we will see an outcome less than or equal to some value $x$.

\pause

$$
F(x) = \textrm{P}[X \le x]
$$

\pause

For our coin flip example,


$$
F(x) = \begin{cases}
0 & x < 0 \\
1/4 & 0 \le x < 1 \\
3/4 & 1 \le x < 2 \\
1 & x \ge 2
\end{cases}
$$

\pause

CDFs are really useful, because if we know the CDF, we can fully describe the distribution of \textit{any} random variable.

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}[fragile]{Illustrating the CDF of a RV}


\begin{figure}
\centering
<<fig = TRUE, width = 5, height=3.8, echo=FALSE>>=
ggplot(plotdata, aes(x = x, y = Fx)) +
  geom_segment(aes(x = x, y = Fx, xend = xend, yend = Fx)) +
  geom_point() +
  geom_point(aes(x = xend, y = Fx), shape= 21, fill = 'white') +
  coord_cartesian(xlim = c(-0.5, 2.5),
                  ylim = c(0,1)) +
  ggtitle('CDF of X as number of heads in 2 fair coin flips') + theme_bw()
@
\end{figure}

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}[fragile]{Illustrating the CDF of a RV}

\scriptsize
And we can use \texttt{ggplot2} to see what the \textit{Empirical} CDF looks like
\tiny
\begin{figure}
\centering
<<fig = TRUE, width = 5, height=3.5>>=

ggplot(data.frame(result_n), aes(x = result_n)) +
  stat_ecdf() +
  coord_cartesian(xlim = c(-0.5, 2.5)) +
  ylab('Empirical Fx') +
  ggtitle('ECDF of X as number of heads in 2 fair coin flips') + theme_bw()

@
\end{figure}

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}[c]

\Large
\centering
\textcolor{Maroon}{Joint and conditional relationships}


\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}{Bivariate relationships}


We often care about how random variables vary with each other
\begin{itemize}
\item age and voter turnout
\item sex and income
\item education and earnings
\end{itemize}

\pause

Just like with univariate random variables, we can describe these bivariate relationships by their distributions

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}{Joint PMF of discrete random variables}

$$
f(x,y) = \textrm{P}[X=x, Y=y]
$$

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}

Returning to our example of flipping two fair coins
\begin{itemize}
\item Let $X$ be 1 if we get \textit{at least one heads}, and 0 otherwise
\item Let $Y$ be 1 if we get \textit{two} heads in our two coin flips, and 0 otherwise
\end{itemize}

\pause

Then the joint probability distribution can be described as:

\begin{table}[]
\begin{tabular}{llll}
$\omega$ & $\textrm{P}[\{\omega\}]$ & $X(\omega)$ & $Y(\omega)$ \\
TT & 1/4 & 0 & 0 \\
TH & 1/4 & 1 & 0 \\
HT & 1/4 & 1 & 0 \\
HH & 1/4 & 1 & 1
\end{tabular}
\end{table}

or, considering the joint PMF,


$$
f(x, y) = \begin{cases}
1/4 & x = 0, y = 0 \\
1/2 & x = 1, y = 0 \\
1/4 & x = 1, y = 1 \\
0 & \text{otherwise}
\end{cases}
$$

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}[fragile]


\scriptsize

<<>>=
Omega <- c('HH', 'HT', 'TH', 'TT')
probs <- c(0.25, 0.25, 0.25, 0.25)

result_n <- sample(x = Omega,
                   size = n,
                   prob = probs,
                   replace = TRUE)

result_mat <- data.frame(omega = result_n,
                         x = ifelse(result_n == 'TT', 0, 1),
                         y = ifelse(result_n == 'HH', 1, 0))

options <- list(theme(panel.grid.minor = element_blank()), 
                # save some style options
                scale_x_continuous(breaks = c(0, 1))) + 
  theme_bw()

p1 <- ggplot(result_mat) + 
  geom_histogram(aes(x = x), bins = 3, 
                 position = 'identity', 
                 color = 'white') + 
  options

p2 <- ggplot(result_mat) + 
  geom_histogram(aes(x = y), 
                 bins = 3, 
                 position = 'identity', 
                 color = 'white') + 
  options

@

\end{frame}

%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}[fragile]
\tiny
<<fig = TRUE, width = 6, height=3.5>>=
grid.arrange(p1, p2, ncol = 2)
@


\small

\pause

Seeing $X$ and $Y$ plotted side by side doesn't really give us a full picture of their relationship.
\pause

These are the \textit{marginal} distributions of $X$ and $Y$, i.e., their distributions where we \textit{marginalize} or sum over the distribution of the other random variable.

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}{Marginal distributions}


$$
f_X(x) = \textrm{P}[X = x]=\sum_y \textrm{P}[X=x, Y=y] = \sum_yf_{X, Y}(x,y)
$$

\pause

\begin{table}[]
\begin{tabular}{cccc}
& $Y = 0$ & $Y = 1$ &  \\
$X = 0$ & $1/4$ & $0$ &$\bm{1/4}$ \\
$X = 1$ & $1/2$ & $1/4$ &$\bm{3/4}$ \\
& $\bm{3/4}$ &$\bm{1/4}$ &
\end{tabular}
\end{table}

\pause

\textit{Notational aside: we can subscript $X$ in $f_X$ to denote that it is the mass function of $X$ specifically, as $X$ and $Y$ have different probability mass functions. But often we will just omit the subscript for convenience.}


\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}[fragile]

\begin{figure}
\centering
\resizebox{.5\textwidth}{!}{
<<fig = TRUE, width = 6, height=6, echo=FALSE>>=
hist_top <- p1
empty <- ggplot() + geom_point(aes(1,1), colour="white") +
  theme(axis.ticks=element_blank(),
        panel.background=element_blank(),
        axis.text.x=element_blank(), axis.text.y=element_blank(),
        axis.title.x=element_blank(), axis.title.y=element_blank())

count_mat <- aggregate(
  list(count = result_mat$omega), 
  list(x = result_mat$x, y = result_mat$y), length)

scatter <- ggplot(result_mat, aes(x = x, y = y, color = omega)) +
  geom_jitter(width = 0.25, height = 0.25, alpha = 0.5) +
  scale_x_continuous(breaks = c(0, 1)) +
  scale_y_continuous(breaks = c(0,1)) +
  theme(panel.grid.minor = element_blank(), legend.position = 'none')

hist_right <- p2 + coord_flip()

grid.arrange(hist_top, empty, scatter, hist_right, ncol=2, nrow=2, widths=c(4, 2), heights=c(2, 4))

@
}
\end{figure}

Plotting $X$ and $Y$ jointly gives us a better understanding of their joint relationship.

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}{Conditional distributions}

We are also often interested in conditional relationships.

$$
f_{Y|X}(y|x) = \textrm{P}[Y = y | X = x] = \frac{\textrm{P}[X=x, Y=y]}{\textrm{P}[X=x]}  = \frac{f_{X,Y}(x,y)}{f_X(x)}
$$


\pause
For example,


$$
f_{Y|X}(y|x) = \begin{cases}
1 & x = 0, y = 0 \\
2/3 & x = 1, y =0  \\
1/3 & x = 1, y = 1 \\
0 & \text{otherwise}
\end{cases}
$$
\pause

Here, what is the probability of observing two heads, conditional on having observed at least one heads?

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}

\Large
\centering
\textcolor{Maroon}{Summarizing single variable distributions}

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}{Expectation}


\pause

$$
\textrm{E}[X] = \sum_x x f(x)
$$

\pause

\begin{itemize}
\item Expectation is an \textit{operator} on a random variable; it maps the distribution of $X$ to a specific number. \pause
\item Specifically, the expectation operator tells us about the mean, or average value of $X$ across its distribution.
\end{itemize}

\pause

\textit{Notational aside: it is common to write the expectation of a distribution as $\mu$.}

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}

Let's flip a single coin, and let $X$ be 1 if we get a head, and 0 otherwise.

\pause


$$
f(x) = \begin{cases}
1/2 & x = 0 \\
1/2 & x = 1 \\
0 & \text{otherwise}
\end{cases}
$$
\pause

Mathematically,


\begin{align*}
\textrm{E}[X] & = \sum_x x f(x)\\
& = 0 \times \frac{1}{2} + 1 \times \frac{1}{2}\\
& = \frac{1}{2}
\end{align*}


\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}[fragile]

Visually,

\begin{figure}
\centering
\resizebox{.75\textwidth}{!}{
<<fig = TRUE, width = 5, height=5, echo=FALSE>>=
plotdata <- data.frame(
  x = c(0, 1),
  fx = c(1/2, 1/2)
)

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.5, 1.5),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0), xend = x, yend = fx)) +
  geom_vline(xintercept = 0.5, lty = 'dashed', color = 'skyblue') +
  annotate(geom="text", x=0.5, y=0.75, label="E[X]") +
  ggtitle('PMF of X as number of heads in 1 fair coin flip')
@
}
\end{figure}

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}{Spread of a distribution}


We often describe the spread of a distribution by its variance

\begin{align*}
\textrm{Var}[X] & = \textrm{E}[(X - \textrm{E}[X])^2]
\end{align*}

Or equivalently,

\begin{align*}
& = \textrm{E}[X^2]-\textrm{E}[X]^2
\end{align*}

\pause

The standard deviation is the square root of the variance.

\pause

\textit{Notational aside: it is common to write the variance of a distribution as $\sigma^2$, or the standard deviation as $\sigma$.}

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
% \begin{frame}
% <!-\item For the variance in our example, we'll use the formula, -->
% 
% <!-\item \begin{align*} -->
% <!-\item \textrm{Var}[X] & = \textrm{E}[X^2]-\textrm{E}[X]^2 \\ -->
% <!-\item \end{align*} -->
% <!-\item $\textrm{E}[X]^2$ is $\textrm{E}[X] \times \textrm{E}[X] = \frac{1}{2} \times \frac{1}{2} = \frac{1}{4}$.  -->
% 
% <!-\item -\item -->
% 
% <!-\item We get $\textrm{E}[X^2]$ in a similar way to how we got the expectation: -->
% <!-\item \begin{align*} -->
% <!-\item \textrm{E}[X^2] & = \sum_x x^2 f(x)\\ -->
% <!-\item & = 0^2 \times \frac{1}{2} + 1^2 \times \frac{1}{2}\\ -->
% <!-\item & = \frac{1}{2} -->
% <!-\item \end{align*} -->
% 
% <!-\item -\item -->
% 
% <!-\item Putting it together, -->
% 
% <!-\item \begin{align*} -->
% <!-\item \textrm{Var}[X] & = \textrm{E}[X^2]-\textrm{E}[X]^2 \\ -->
% <!-\item & = \frac{1}{2} \item \frac{1}{4}\\ -->
% <!-\item & = \frac{1}{4} -->
% <!-\item \end{align*} -->
% 
% <!-\item \end{frame}
% 
% 
% %%%%%NOTE%%%%%
% 
% \note{
% \scriptsize \singlespacing
% 
% \begin{itemize}
% \item xxxx
% \end{itemize}
% 
% }
% 
% 
%-------------------------------------------------------------------------------%
\begin{frame}[fragile]
The variance is the average squared distance from the mean. The standard deviation is the square root of this.


\begin{figure}
\centering
\resizebox{.7\textwidth}{!}{
<<fig = TRUE, width = 5, height=5, echo=FALSE>>=

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.5, 1.5),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0), xend = x, yend = fx)) +
  geom_vline(xintercept = 0.5, lty = 'dashed', color = 'skyblue') +
  annotate(geom="text", x=0.5, y=0.75, label="E[X]", col = 'grey') +
  geom_segment(aes(x = 0.5, xend = 0.0, y = 0.5, yend = 0.5), arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_segment(aes(x = 0.5, xend = 1, y = 0.5, yend = 0.5), arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  annotate(geom="text", x=0.75, y=0.58, label="Distance\nfrom mean") +
  annotate(geom="text", x=0.25, y=0.45, label="-0.5", color = 'skyblue') +
  annotate(geom="text", x=0.75, y=0.45, label="0.5", color = 'skyblue') +
  geom_point(aes(x = 0.5, y = 0.5), color = 'skyblue') +
  ggtitle('PMF of X as number of heads in 1 fair coin flip')
@
}
\end{figure}

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}[fragile]
The variance is the average squared distance from the mean. The standard deviation is the square root of this.

\begin{figure}
\centering
\resizebox{.7\textwidth}{!}{
<<fig = TRUE, width = 5, height=5, echo=FALSE>>=

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.5, 1.5),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0), xend = x, yend = fx)) +
  geom_vline(xintercept = 0.5, lty = 'dashed', color = 'skyblue') +
  annotate(geom="text", x=0.5, y=0.75, label="E[X]", col = 'grey') +
  geom_segment(aes(x = 0.5, xend = 0.25, y = 0.5, yend = 0.5), arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_segment(aes(x = 0.5, xend = 0.75, y = 0.5, yend = 0.5), arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  annotate(geom="text", x=0.75, y=0.58, label="Squared distance\nfrom mean") +
  annotate(geom="text", x=0.25, y=0.45, label="0.25", color = 'skyblue') +
  annotate(geom="text", x=0.75, y=0.45, label="0.25", color = 'skyblue') +
  geom_point(aes(x = 0.5, y = 0.5), color = 'skyblue') +
  ggtitle('PMF of X as number of heads in 1 fair coin flip')
@
}
\end{figure}

Variance = $0.25\times 0.5 + 0.25\times 0.5 = 0.25$

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}
The variance is the average squared distance from the mean. The standard deviation is the square root of this.

\begin{figure}
\centering
\resizebox{.7\textwidth}{!}{
<<fig = TRUE, width = 5, height=5, echo=FALSE>>=

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.5, 1.5),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0), xend = x, yend = fx)) +
  geom_vline(xintercept = 0.5, lty = 'dashed', color = 'skyblue') +
  annotate(geom="text", x=0.5, y=0.75, label="E[X]", col = 'grey') +
  geom_segment(aes(x = 0.5, xend = 0.0, y = 0.5, yend = 0.5), arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_segment(aes(x = 0.5, xend = 1, y = 0.5, yend = 0.5), arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  annotate(geom="text", x=0.75, y=0.6, label="Square root of average\nsquared distance\nfrom mean") +
  annotate(geom="text", x=0.25, y=0.45, label="0.5", color = 'skyblue') +
  annotate(geom="text", x=0.75, y=0.45, label="0.5", color = 'skyblue') +
  geom_point(aes(x = 0.5, y = 0.5), color = 'skyblue') +
  ggtitle('PMF of X as number of heads in 1 fair coin flip')
@
}
\end{figure}

SD = $\sqrt{0.25} = 0.5$

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}

Let's take another example, where we flip a coin twice, and let $X$ be the number of heads. However, let's say our coin is \textit{not} fair, and the probability of getting a heads is 0.75.\\~\

\pause

The random variable's probability distribution is then:

$$
f(x) = \begin{cases}
1/16 & x = 0 \\
3/8 & x = 1 \\
9/16 & x = 2 \\
0 & \text{otherwise}
\end{cases}
$$

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}
\begin{minipage}[t][-0.01\textheight][t]{\textwidth}
\footnotesize
Let's take a look at the mean.
\end{minipage}

\begin{minipage}[t][0.62\textheight][t]{\textwidth}
\begin{figure}
\centering
\resizebox{.55\textwidth}{!}{
<<fig = TRUE, width = 5, height=5, echo=FALSE>>=
plotdata <- data.frame(
  x = c(0, 1, 2),
  xend = c(1, 2, 3),
  fx = c(1/16, 3/8, 9/16),
  Fx = cumsum(c(1/16, 3/8, 9/16))
)

# Expected value
Ex <- sum(plotdata$x*plotdata$fx)

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.8, 2.8),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0,0), xend = x, yend = fx)) +
  geom_vline(xintercept = 1.5, lty = 'dashed', color = 'skyblue') +
  annotate(geom="text", x=1.5, y=0.75, label="E[X]") +
  ggtitle('PMF of X as number of heads in 2 UNfair coin flips')
@
}
\end{figure}
\end{minipage}


\begin{minipage}[t][0.4\textheight][t]{\textwidth}
\footnotesize
\begin{align*}
\textrm{E}[X] & = \sum_x x f(x) \\
& = 0 \times \frac{1}{16} + 1 \times \frac{3}{8} + 2 \times \frac{9}{16}\\
& = \frac{24}{16} = 1.5
\end{align*}
\end{minipage}


\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}[fragile]
\begin{minipage}[t][-0.01\textheight][t]{\textwidth}
\footnotesize
And the spread.
\end{minipage}

\begin{minipage}[t][0.68\textheight][t]{\textwidth}

\begin{figure}
\centering
\resizebox{.55\textwidth}{!}{
<<fig = TRUE, width = 5, height=5, echo=FALSE>>=
ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.8, 2.8),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0,0), xend = x, yend = fx)) +
  geom_vline(xintercept = Ex, lty = 'dashed', color = 'skyblue') +
  annotate(geom="text", x=Ex, y=0.75, label="E[X]", color = 'grey') +
  geom_segment(aes(x = Ex, xend = x, y = fx, yend = fx),
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_point(aes(x = Ex, y = fx), color = 'skyblue') +
  annotate(geom="text", x=1.8, y=0.63, label="Distance\nfrom mean") +
  annotate(geom="text", x=(plotdata$x+Ex)/2, y=(plotdata$fx-0.05), label=(plotdata$x-Ex), color = 'skyblue') +
  ggtitle('PMF of X as number of heads in 2 UNfair coin flips')
@
}
\end{figure}
\end{minipage}

\begin{minipage}[t][0.4\textheight][t]{\textwidth}
\footnotesize
Variance = average squared distance from the mean
\begin{align*}
\textrm{Var}[X] & = \textrm{E}[(X - \textrm{E}[X])^2]\\
&\color{white}{ = 2.25 \times \frac{1}{16} + 0.25 \times \frac{3}{8} + 0.25 \times \frac{9}{16} = 0.375}
\end{align*}
\end{minipage}





\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}[fragile]
\begin{minipage}[t][-0.01\textheight][t]{\textwidth}
\footnotesize
And the spread.
\end{minipage}

\begin{minipage}[t][0.68\textheight][t]{\textwidth}

\begin{figure}
\centering
\resizebox{.55\textwidth}{!}{
<<fig = TRUE, width = 5, height=5, echo=FALSE>>=

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.8, 2.8),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0,0), xend = x, yend = fx)) +
  geom_vline(xintercept = Ex, lty = 'dashed', color = 'skyblue') +
  annotate(geom="text", x=Ex, y=0.75, label="E[X]", color = 'grey') +
  geom_segment(aes(x = Ex, xend = Ex+sign(x-Ex)*(x-Ex)^2, y = fx, yend = fx),
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_point(aes(x = Ex, y = fx), color = 'skyblue') +
  annotate(geom="text", x=1.8, y=0.63, label="Squared distance\nfrom mean") +
  annotate(geom="text", x=(plotdata$x+Ex)/2, y=(plotdata$fx-0.05), label=(plotdata$x-Ex)^2, color = 'skyblue') +
  ggtitle('PMF of X as number of heads in 2 UNfair coin flips')
@
}
\end{figure}
\end{minipage}

\begin{minipage}[t][0.4\textheight][t]{\textwidth}
\footnotesize
Variance = average squared distance from the mean
\begin{align*}
\textrm{Var}[X] & = \textrm{E}[(X -\textrm{E}[X])^2]\\
&{ = 2.25 \times \frac{1}{16} + 0.25 \times \frac{3}{8} + 0.25 \times \frac{9}{16} = 0.375}
\end{align*}
\end{minipage}


\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}[fragile]
\begin{minipage}[t][-0.01\textheight][t]{\textwidth}
\footnotesize
And the spread.
\end{minipage}

\begin{minipage}[t][0.68\textheight][t]{\textwidth}

\begin{figure}
\centering
\resizebox{.55\textwidth}{!}{
<<fig = TRUE, width = 5, height=5, echo=FALSE>>=

sdx <- sqrt(sum((plotdata$x-Ex)^2 * plotdata$fx))

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.8, 2.8),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0,0), xend = x, yend = fx)) +
  geom_vline(xintercept = Ex, lty = 'dashed', color = 'skyblue') +
  annotate(geom="text", x=Ex, y=0.75, label="E[X]", color = 'grey') +
  geom_segment(aes(x = Ex, xend = Ex-sdx, y = 0.5, yend = 0.5),
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_point(aes(x = Ex, y = fx), color = 'skyblue') +
  geom_segment(aes(x = Ex, xend = Ex+sdx, y = 0.5, yend = 0.5),
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_point(aes(x = Ex, y = fx), color = 'skyblue') +
  annotate(geom="text", x=1.8, y=0.63, label="Square root of average\nsquared distance\nfrom mean") +
  annotate(geom="text", x=(Ex+c(-1.05,1.05)*round(sdx, 3)/2), y=0.45,
           label=round(sdx, 3), color = 'skyblue') +
  ggtitle('PMF of X as number of heads in 2 UNfair coin flips')
@
}
\end{figure}
\end{minipage}

\begin{minipage}[t][0.4\textheight][t]{\textwidth}
\footnotesize


SD = square root of variance
$$
= \sqrt{0.375} = 0.612
$$
\end{minipage}

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}{Applications}


\begin{itemize}
\item Coin flips are a pretty trivial example of a random event $\rightarrow$ random variable.\pause
\item But often, as researchers, our job is to map events that happen in the world to variables in our data sets.
\end{itemize}

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}\Large
\centering
\textcolor{Maroon}{Summarizing joint distributions}

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}{Covariance}

$$
\textrm{Cov}[X,Y] = \textrm{E}[(X - \textrm{E}[X])(Y-\textrm{E[Y]})]
$$

\pause

Covariance is how much $X$ and $Y$ vary together.

\pause
\begin{itemize}
\item If covariance is positive, when the value of $X$ is large (relative to its mean), the value of $Y$ will also tend to be large (relative to its mean)
\item If covariance is negative, when the value of $X$ is large (relative to its mean), the value of $Y$ will tend to be small (relative to its mean)
\end{itemize}


\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\begin{frame}{Correlation}

$$
\rho[X, Y] = \frac{\textrm{Cov}[X,Y]}{\sigma[X] \sigma[Y]}
$$

Rescaled version of covariance
\begin{itemize}
\item positive when covariance is positive
\item negative when covariance is negative
\end{itemize}

\pause

$$
-1 \le \rho[X, Y]  \le 1
$$

\end{frame}


%%%%%NOTE%%%%%

\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}


%-------------------------------------------------------------------------------%
\backupbegin
%-------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{References}
    \bibliographystyle{apalike}
    \bibliography{../assets/bib}
    
\end{frame}
%-------------------------------------------------------------------------------%
\begin{frame}[label = event_space]{Flipping two coins event space:}


\begin{align*}
S & = \{\emptyset, \\
&\{HH\}, \{HT\},\{TH\}, \{TT\},\\
&\{HH, HT\}, \{HH, TH\},\{HH, TT\}, \{HT, TH\}, \{HT, TT\}, \{TH, TT\}\\
& \{HH, HT, TH\}, \{HH, HT, TT\}, \{HH, TH, TT\}, \{HT, TH, TT\},\\
&\{HH, HT, TH, TT\} \}
\end{align*}

\hyperlink{terms}{\beamerbutton{Back to terms}}

\end{frame}
%-------------------------------------------------------------------------------%


\backupend
\end{document}
%
%-------------------------------------------------------------------------------%

%%% [[TEMPLATE]] %%%
\begin{frame}{Frametitle}

\begin{itemize}
\item xxx
\end{itemize}

\end{frame}


%%%%%NOTE%%%%%
\note{
\scriptsize \singlespacing

\begin{itemize}
\item xxxx
\end{itemize}

}

%-------------------------------------------------------------------------------%

<<echo = FALSE>>=
f <- 'slides_31.Rnw'
knitr::purl(f)
knitr::Sweave2knitr(f)
@